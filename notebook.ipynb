{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a5d6e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import os\n",
    "from transformers import pipeline\n",
    "import pywhisper\n",
    "import torch\n",
    "import av\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import LlavaNextVideoProcessor, LlavaNextVideoForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcripción y resumen de audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n",
      "GPU disponible?: False\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "print(\"GPU disponible?:\", torch.cuda.is_available())\n",
    "\n",
    "def transcribir_audio(archivo_audio):\n",
    "    modelo = pywhisper.load_model(\"base\", device=device)  # Puedes ajustar el modelo según tus necesidades\n",
    "    resultado = modelo.transcribe(archivo_audio)\n",
    "    return ' '.join(dic['text'] for dic in resultado['segments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4466223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "def summarize_long_text(text, chunk_size=512):\n",
    "    \"\"\"Summarizes a long text by splitting it into chunks.\"\"\"\n",
    "    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    summary_chunks = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        summary_chunks.append(summarizer(chunk, max_length=130, min_length=30, do_sample=False)[0]['summary_text'])\n",
    "\n",
    "    return \" \".join(summary_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " El tema de los buscadores de cada vez, que es una tema muy heavy, que se que no podés  mostrar en YouTube tal cual lo viviste, pero ¿qué es esto? ¿Qué significa hasta  los buscadores de cada vez para la gente que no lo sabe?  Platzard, que antes se llamaba los tulipanes negros, que lo verán, son un grupo que  medio pena no haberlo explicado mejor en la pieza en el capítulo.  Ahora puedes espalhar de aquí.  Es algo muy sencillo, que son civiles, no son militares.  No, si hay, tenemos un gráfico, un rótulo que hablamos que son voluntarios como forma  de esa, nuestra idea es explicar que son civiles, porque para mí me parecía de hecho una  parte clave de la historia, que son civiles, pasas que van vestidos como militares,  van de camuflaje y con su casco, con su será levantivalas, por el objetivo metácelo  en el labor en zona guerra.  Entonces aglarar que son civiles, que me parece todavía más fuertes civiles, voluntarios  que se meten en las zonas, campos repletos de minas, el protagonista de este capítulo  me parece brutal la visión que tiene como habla de las almas y además no hace distinción  de Ukrainianos y de rusos.  De hecho, cuando encuentran restos humanos, ¿por qué van a pasar de constar?  ¿Qué son? ¿Qué va buscando cada vez? ¿Vos van a buscar cada vez?  Vos cada vez de cada vez.  Y los dejan en algún sitio que concreto, como se repatrian, los envían a casa tanto  Ukrainianos como rusos.  ¿Enterno van a hospital o a la morgue, donde los llevan estos?  Ellos tienen una centraita, como una sede, que es una casa en el campo, ahí tienen camiones  de frío, que es de donde sejaron los cadáveres, cuando fui al bosque con ellos no encontramos  cadáveres y no restos humanos, había, tenían pintas de ser, bueno, no sé, a ver, junto  a los restos humanos, había un femur, había un montón de huesos, los recogen también.  ¿Y eso?  ¿Y eso?  ¿Y eso?  ¿Y eso?  ¿Y eso?  ¿Y eso?  ¿Y eso?  ¿Y eso?  ¿Y eso?  ¿Y eso?  ¿Y eso?  ¿Y eso?  ¿Y eso?  ¿Y eso?  ¿Y eso?  ¿Y eso?  ¿Y eso?  ¿Y eso?  Pues él decía que no sé si eran rusos, no, si es cierto que haya lángut, tenemos un  gorro ruso.  Chamusca o esa saber, eso era lo que había sido el frente de dos años atrás, era todo  chamusca o de los misiles, los árboles, negros, los que quedaban, o sea, el manto de  nieve porque estamos en invierno, pero con trozos de tanques, con trozos de cañones,  sí, era una estampa.  Claro que, si hablamos de los olores, eso es realmente también ayuda a que se va  el frío. Pero lo que te digo, o sea, restos humanos como tal, yo eso eran huesos y los  cadáveres también habían tres rusos que habían atrapado y que no habían salado a la vida.  De hecho, estaba en un segundo tenía un tromadismo en la cabeza, o todos le había contó  ríguetes en las piernas. Me sorprendió también, y esto se ve en el reportaje, porque en el  lado de Ucrania, enseño, no es por ir con druganías, porque este no es una realidad,  es que hay una Ucrania. Se han llevado no voy ni a favor ni uno ni de otro en los reportajes,  yo puedo tener mi ideología, puedo tener un pesamiento, no lo plasmo en mis trabajos.  O sea, es de un punto de vista humano. Entonces, en el tercer capítulo como digo, hablo de los  reclutas, de los mayores que son, las realidades que esos cadáveres rusos tan poderán niños.  En una edad avanzada y estaban muy dolgados, soblando de la situación que ella también  en el otro lado del frente. Entonces, me hace ver que realmente están viviendo muy similanes.  O sea, yo es lo que digo, a nivel humano. No si se puede comparar, pero a nivel humano,  en Rusia también se recluta de manera forzosa, también hay padres de familia que no quieren  estar y que están allí, se les suficienta como carne picada en el frente. Entonces, lo que  digo, desde un punto de vista humano, realmente a nivel de esas realidades y como plasma también,  el protagonista de buscadores de cadáveres, es el Campítulo 2, es la primera realidad,  después de la introducción, la primera realidad que se documenta. A nivel humano no  se hace esa diferencia, o sea, realmente, según otras situaciones muy similares, a nivel,  de hombres de familia, de hombres de familia, de chavalitos de 18 años que están ahí.  Entonces, hay una gran espultad dentro de los míos para mí, de ese capítulo, me parece  brutal el protagonista. Esa es la visión que tiene el protagonista, como habla, también  me dejaba, yo me enteraba de cositas porque nos iba introduciendo un poquito sobre la marcha.  Bueno, ya nos dijo con el que introducíamos, que era con el fixer, ¿no? ¿Qué hacía  como interpreter? Y nos decía, es muy fuerte lo que está hablando, ya lo veréis. Nos dimos  cuenta realmente aquí a posterior de traduciendo, ya haciendo lo que más impactó de lo que dijo.  Bueno, eso porque tiene una vista, tiene una visión súper espiritual de todo aquello y habla  de lo cargado que está se bosque con las almas de los que han muerto ahí. Y él tiene también  una visión muy caro, el vaver restos y él siente, la habla que siente que le llaman,  no sé, que siente más de una vez, lo que me dijeron, porque yo le vi, a ver, nosotros  llegamos a un lugar donde ya estaban marcas, o sea, no íbamos a buscar donde pueda ver  y vamos bajo aviso, había unas cintas, bajo aviso que hay restos en ese sitio. Entonces,  si van a inspeccionar la zona un poquito, pero no te mueves mucho porque está todo lleno  de minas. Vamos andando, pisando lo que pisan de adelante. Entonces, y me decir eso,  se recojan esos huesos y luego yo vi que él se ponía de rodillas a inspeccionar por otras  zonas y tal. Y sí que se broma contaron que él muchas veces encuentra huesos que no están  marcados, aquí había que ir quitando nieve y en el mundo, como una sensibilidad o como  un sensibilidad o no digamos. O al menos lo que te digo, ese a verdad que no todo está  en cómo él lo viva, lo que o sea cómo él lo vive, le hace buscar más allá, se mete  en caminos donde nos ha señalado nada, encuentra más allá, que es porque lo ha sentido  de verdad, o que es porque es el que se mete más, bueno, el cobran por esto, no cobran,  son voluntarios, hay donaciones, pero se hay gente tiene un trabajo aparte de esto o se dedican  fior en la guerra, estaban 100% dedicados. Entonces, claro, como que algún sitio deben de  algo de las donaciones, las donaciones, pues se puede donar a plas darn, que es el que  es el grupo, esta que ves, yo también creo que, oye, si le va bien la espiritualidad para  esto, porque al final tú sabes lo duro que tiene que ser para una persona, estás recogiendo  cada vez recientes durante meses de niños, de ancianos, de todo, de víctimas civiles  que también por desgracia encuentran.\n"
     ]
    }
   ],
   "source": [
    "transcripcion = transcribir_audio(\"./grabaciones/podcast.mp3\")\n",
    "print(transcripcion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 130, but your input_length is only 4. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Platzard\" es un grupo que  medio pena no haberlo explicado mejor. Es algo muy sencillo, que son civiles, no son militares. No, if hay, tenemos un gráfico, un rótulo q\" \"Ue hablamos que son voluntarios como forma  de esa, nuestra idea es explicar that son civiles, porque para mí me parece de hecho una  parte clave of la historia, que sonCiviles, pasas que van vestidos como militares,  van de camuflaje\" Parece brutal la visión que tiene como habla de las almas. No hace distinción  de Ukrainianos y de rusos. Ellos tienen una centraita, como una sede, que es u u \"No sé, a ver, junto  a los restos humanos, había un femur\" \"Pues él decía que no sé if eran rusos\" Chamusca o esa saber, eso era lo que había sido el frente de dos años atrás, era todo  chamusca or de los misiles, los árboles, negros, los that quedaban, o sea, el manto de  nieve porque estamos en invierno, pero with trozos de tanques, con trozos of cañones,  sí, era una estampa. Claro que, if hablamos de los olores, ese es realmente Adáveres también habían tres rusos que había atrapado. De hecho, estaba en un segundo tenía un tromadismo in la cabeza. \"Yo puedo tener mi ideología, puede tener un pesamiento, no l \"Me hace ver que realmente están viviendo muy similanes. O sea, yo es lo que digo, a nivel humano. No si se puede comparar\" El Campítulo 2, es la primera realidad,  después of la introducción, la primero realidad que se documenta. A nivel humano no  se hace esa diferencia, o sea, según otras situaciones muy similare. luta de manera forzosa. \"Me parece  brutal el protagonista. Esa es la visión que tiene el protagonistista, como habla, también  me dejaba\" \"Nos dimos  cuenta realmente aquí a posterior de traduciendo, ya haciendo lo that más impactó of lo that dijo. Bueno, eso porque tiene  una vista, tiene una visión súper espiritual de todo aquello\" \"No te mueves mucho porque está todo lleno  de minas. Vamos andando, pisando lo that pisan de adelante. Entonces, y me decir eso,  se recojan esos huesos\" No todo está en cómo él lo viva, lo que o sea cómolo lo vive, le hace buscar más allá, se mete  en caminos donde nos ha señalado nada, encuentra m more allá. Es porque lo ha sentido  de verdad, o que es porque es el that se mete más, bueno, el cobran por esto, no cobran,  son voluntarios. \"Algún sitio deben de  algo de las donaciones, lasDonaciones\" \"Se puede donar a plas darn, que es el que  es el grupo, esta that ves\" An. an. an an. a.    “I’m sorry I didn’t get your name right.” “What’s your name?” you ask. “I want to know what your name is.’ ““”“What do you say to me?’ I’ve got a question for you.“ “Can you tell me what you think?“,” he asks. ”What do I say to you? ”.\n"
     ]
    }
   ],
   "source": [
    "resumen = summarize_long_text(transcripcion)\n",
    "print(resumen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcripción de grabación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3721b543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comienzo de la grabación...\n",
      "Fin de la grabación...\n",
      "Grabacion completadas.\n"
     ]
    }
   ],
   "source": [
    "FRAMES=1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 44100\n",
    "RUTA_GRABACIONES= \"./grabaciones\"\n",
    "os.makedirs(RUTA_GRABACIONES, exist_ok=True)\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "stream = p.open(\n",
    "                format=FORMAT,\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                frames_per_buffer=FRAMES\n",
    "                )\n",
    "print(f\"Comienzo de la grabación...\")\n",
    "frames=[]\n",
    "segundos=10\n",
    "for j in range(0,int(RATE/FRAMES*segundos)):\n",
    "    data=stream.read(FRAMES)\n",
    "    frames.append(data)\n",
    "\n",
    "print(f\"Fin de la grabación...\")\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "filename = os.path.join(RUTA_GRABACIONES, f\"grabacion.wav\")\n",
    "with wave.open(filename, 'wb') as wf:\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(2)  \n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "\n",
    "p.terminate()\n",
    "\n",
    "print(\"Grabacion completadas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dde1bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rafae\\Desktop\\AINE_AUDIO_SUMMARIZE\\.venv\\Lib\\site-packages\\pywhisper\\transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pero a un altar de mocha...  ¡Octía mierda!  Que estoy cambiando la grabación del otro día.\n"
     ]
    }
   ],
   "source": [
    "transcripcion = transcribir_audio(\"./grabaciones/grabacion.wav\")\n",
    "print(transcripcion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 130, but your input_length is only 40. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pero a un altar de mocha...  ¡Octía mierda!  Que estoy cambiando la grabación del otro día.\n"
     ]
    }
   ],
   "source": [
    "resumen = summarize_long_text(transcripcion)\n",
    "print(resumen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcripción de video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.92s/it]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"llava-hf/LLaVA-NeXT-Video-7B-hf\"\n",
    "\n",
    "model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    low_cpu_mem_usage=True, \n",
    ").to(device)\n",
    "\n",
    "processor = LlavaNextVideoProcessor.from_pretrained(model_id)\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "\n",
    "# define a chat history and use `apply_chat_template` to get correctly formatted prompt\n",
    "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\", \"video\") \n",
    "conversation = [\n",
    "    {\n",
    "\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Describe el video. Debes responder en español.\"},\n",
    "            {\"type\": \"video\"},\n",
    "            ],\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "def transcribir_video(video_path):\n",
    "  container = av.open(video_path)\n",
    "  total_frames = container.streams.video[0].frames\n",
    "  indices = np.arange(0, total_frames, total_frames / 8).astype(int)\n",
    "  clip = read_video_pyav(container, indices)\n",
    "  inputs_video = processor(text=prompt, videos=clip, padding=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "  output = model.generate(**inputs_video, max_new_tokens=100, do_sample=False)\n",
    "  \n",
    "  transcription = processor.decode(output[0][2:], skip_special_tokens=True)\n",
    "  return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"./grabaciones/papa.mp4\"\n",
    "transcripcion = transcribir_video(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER: \n",
      "Describe el video. Debes responder en español. ASSISTANT: Lo sient\n"
     ]
    }
   ],
   "source": [
    "print(transcripcion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 130, but your input_length is only 28. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery. Visit CNN.com/Travel each week for a new gallery of snapshots from around the world.\n"
     ]
    }
   ],
   "source": [
    "resumen = summarize_long_text(transcripcion)\n",
    "print(resumen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validación audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aparicion_palabras_clave(texto, palabras_clave):\n",
    "    conteo = {palabra: texto.lower().count(palabra.lower()) for palabra in palabras_clave}\n",
    "    print(\"Palabras clave que deben aparecer:\", palabras_clave)\n",
    "    print(\"Número de palabras clave:\", len(palabras_clave))\n",
    "    print(\"Conteo de palabras clave que aparecen:\", conteo)\n",
    "    print(\"Número de palabras clave que aparecen:\", sum(1 for count in conteo.values() if count > 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Audio corto de whatsapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 130, but your input_length is only 25. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=12)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bajaro de los Nacionales volando por arriba de la feria de Sevilla\n",
      "Bajaro de los Nacionales volando por arriba de la feria de Sevilla. Bajaro   volando  por ariba de the feria.\n",
      "Palabras clave que deben aparecer: ['feria', 'nacionales']\n",
      "Número de palabras clave: 2\n",
      "Conteo de palabras clave que aparecen: {'feria': 1, 'nacionales': 1}\n",
      "Número de palabras clave que aparecen: 2\n"
     ]
    }
   ],
   "source": [
    "transcripcion = transcribir_audio(\"./grabaciones/audiowhatsapp.opus\")\n",
    "print(transcripcion)\n",
    "resumen = summarize_long_text(transcripcion)\n",
    "print(resumen)\n",
    "\n",
    "palabras_clave = [\"feria\", \"nacionales\"]\n",
    "\n",
    "aparicion_palabras_clave(transcripcion, palabras_clave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Letra de canción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cuando paso por el puente triana contigo mi camina  Cuando paso por el puente triana contigo mi camina triana contigo mi camina  Con mirarte solamente mi camina me muero garevia  Porque tienes unos sombianas y cualquier dulceiro triana y cualquier dulceiro  Y una clase de chuva triana te va en un duplero  Si para otro me deja la triana de pena moriría  Me di a nadie, no te quiero y te mere, me rendido el teniría triana triana y un el  Velaita de Santana triana, farones sobre el río  Velaita de Santana triana, farones sobre el río triana  Falo de sobre el río cuantesitos de avellana triana y tuve el plazo río  Un ritano va cantando triana con promos de colores triana con promos de colores  Una pie a fuego una conriana pétiño y alfrajo  Calentito y busco el otro triana que le gra su pestaña  Ay, niña, que mira, mira, me vamos a ver la cumpaña, se te daña triana y un el  Arfarero de triana triana, casi al nos afina  Alfarero de triana triana, casi al nos afina triana, casi al nos afina  En el cielo hay no herma, la triana que sonca y no digna  El casoro tan abandono triana y dice el que no ha hito triana y dice el que no hito  Un semplante agonizante triana igual al que se hito y eseño sanizador o triana  Reponda la dosa ella la crea también tiene que ver un cavitar a tambien triana triana y tuve  Por barcones y ventana triana, manto nejialería  Por barcones y ventana triana, manto nejialería  La manto nejialería, manto nejialería  Y le pica las cantanas triana de toda la moscilla  No hay poesa que le escriba triana, la moscilla triana, la moscilla triana y ese barrio dando quino triana la vida del cosillo  Refrando sanerío triana, la luna y asasoma y todo repina en la bella, la bella, la blanca paloma triana triana y tuve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 130, but your input_length is only 78. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=39)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para mirarte solamente mi camina me muero garevia. Porque tienes unos sombianas y cualquier dulceiro triana. Y una clase de chuva triana te va en un duplero. Si para otro me deja la triana de pena moriría. Un ritano va cantando triana con promos de colores triana with promos of colores. Una pie a fuego una conriana pétiño y alfrajo. Calentito y busco el otro triana that le gra su pestaña. Triana is un semplante agonizante triana igual al that se hito y eseño sanizador o triana. Reponda la dosa ella la crea también tiene que ver un cavitar a tambien triana triana y tuve. Ana, la moscilla triana, the moscillas triana. ana, la luna y asasoma. ana. Ana, the bella, the blanca paloma triana triana and tuve.\n",
      "Palabras clave que deben aparecer: ['sevilla', 'triana', 'giralda', 'arte', 'hija', 'buleria', 'baile']\n",
      "Número de palabras clave: 7\n",
      "Conteo de palabras clave que aparecen: {'sevilla': 0, 'triana': 38, 'giralda': 0, 'arte': 1, 'hija': 0, 'buleria': 0, 'baile': 0}\n",
      "Número de palabras clave que aparecen: 2\n"
     ]
    }
   ],
   "source": [
    "transcripcion = transcribir_audio(\"./grabaciones/sevillana.mp3\")\n",
    "print(transcripcion)\n",
    "resumen = summarize_long_text(transcripcion)\n",
    "print(resumen)\n",
    "\n",
    "palabras_clave = [\"sevilla\", \"triana\", \"arte\", \"niña\"]\n",
    "\n",
    "aparicion_palabras_clave(transcripcion, palabras_clave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio de video largo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A ver, Flowers de My Liz Iris.  Hace de la canción más estremeada en su primera semana en la historia de Spotify.  O sea, le he ido muy bien.  Dicho es, quiero analizar Flowers porque me gusta mucho y quiero analizar la canción  en la que se basa, que me gusta que creo que aún más que es Buenaigua's Your Man de  Rommar.  La canción de Flowers está basada en la letra y en la melodía y en los acordes.  Hay mucha gente hablando de que ha cogido cosas de la letra.  La letra es una respuesta a la de Bruno Mars, pero la melodía también es la misma.  A gente que se dedica a analizar música y siendo no tiene nada que ver melodicamente.  Hay artículos de Billboard diciendo no, no coge ningún elemento melodico, no es verdad.  Es una canción respuesta en todas las maneras de responder letra, melodía y acordes.  Se falta la producción, la producción coge la de Will's Urmite, entonces pues ya estamos  a peñón.  Capítulo 1, lo que todo el mundo sea.  La letra de My Liz Iris dice,  «I can buy myself flowers and I can hold my own hand».  Esta letra es en respuesta a cómo se sentía Miley en su relación con su ex.  «I don't ever want to have to need someone again where you feel like without them,  you can't be yourself».  Entonces aquí está diciendo, me a comprar mis propias flores, me a escribir mi propio nombre en la arena,  voy a hablar conmigo misma, voy a llevarme a bailar a mi misma, voy a darme la mano a mi misma,  ya está, chao, fuera.  Puedo hacer esas cosas sin ti.  Y aunque eso en principio se lo está diciendo a su ex, está articulándolo como una respuesta,  a una canción de Bruno Mars.  Cantadas desde la perspectiva del chico en la relación, diciendo esto.  Tratar un poco sobre ti.  Toc unas mez clicos, literalmente desconoc Row,  yiencies lo lo antaroque.  Cómo reveals en hello…  Tienos que ver.  Hay rumores sobre si es que Alex de Miley le dedicaba mucho a la canción de Bruno Mars y no si lo que sea.  H get yelled.  L단as estas confirmas.  confirmó. El tema es que es una answer song, una canción que responde a otra canción,  y está formando un diálogo con otra canción a una década de distancia de las canciones.  Pero hay más, no es solo una answer song en letra. Esta es la parte que no he visto  tan comentado. La letra del estribillo de Miley está basada en la letra del estribillo  de Bruno Mars, pero la melodía y los acordes del estribillo de Miley son la estrofe de  la canción de Bruno Mars.  Los acordes son los mismos. Si veinte Bruno adornan algunos acordes metiéndole  a una séptima en medio, porque su canción dura un poco más, porque está todo más dilatado.  Bruno es una esencia también hace la menor re, sol, do y para ir del do, a la menor, hace el  solo, y a la menor, hace prácticamente lo mismo en esencia. Lo que pasa es que lo copierte  en un do directamente, pero porque quita una nota, la melodía es la misma también, eso  es lo que es muy bonito. Lo que hace para mí es que la respuesta sea todavía más potente.  Y si llegates a la misma nota simplemente Miley es más rápido. Bruno hace. Bruno hace así.  Miley es lo mismo muy rápido. Lo único que cambia es el tempo. Miley ha cogido Bruno  Mars, lo ha hecho el doble de rápido y ha cantado lo que cantaba Bruno prácticamente.  Es el tema de Flágo y de Undertale por encima en Flogueres de Miley.  No tiene nada que ver con el vídeo, pero me hacía gracias.  Vale, entonces ¿qué pasa? Que cuando Miley se irucoje los acordes de Bruno Mars y lo  simplifica un poco para cortarlos, le queda la menor, remenor, sol, do. Si sigues un  poco más que tienes la serie de cuartas del barroco. Puedes coger los cuatro primeros  acordes y hacer las cuatro estaciones Miley, Sairos muy fácilmente.  I will Survive tiene esa serie de acordes. Y I will Survive va de una tiba que te  creía que no iba a poder Tarvi M.C. y luego voy y descubro que sí resumen.  Es decir, los cuatro primeros acordes de Miley que ha cogido de Bruno Mars son iguales  que los cuatro primeros acordes de I will Survive. Todos empieza a conectar en su cabeza  y le da una producción estilo I will Survive. Estas son las cuerdas de Will Survive.  Estas son las cuerdas de Flowers. La segunda mitad de la estribuía de Flowers suena muy  parecida a estas cuerdas de I will Survive.  No sé en qué orden empezaron a conectar las ideas, pero es muy fortuito que justo la  canción de Bruno Mars y la de I will Survive tengan estos acordes. Además ellos lo  cogen para Flagways y entonces cogen la producción de I y la letra de otro y la melodía de  Ufink ha quedado pues una cosa muy coherente a nivel artístico. Está como respondiendo  a Bruno Mars con la potencia de la producción de Gloria Gaynor.  Es decir, un ejemplo de cómo hacer un hit viral es este. Si tú trabajas en una disco  gráfica y tú haces Flowers y tú se le enseñas a la R, el artista y repertorio, que es  un puesto de una persona que se supone que ve la por tu éxito profesional en la industria.  Tú le enseñas esta canción a la R y la R te pone un 10. Te pone una matrícula de honor  y es por la estructura. Esta es la estructura de la canción de Flowers. Esta es la estrofa,  pre-estrivillo, estrivillo, primera mitad del estrivillo y segunda mitad del estrivillo.  El estrivillo entra en el segundo 33. Eso es Gold Standard. Eso es en plan. Has conseguido  que la gente se coma el estrivillo en el segundo 33, o sea, en hora buena. Se suele  pedir que por favor el estrivillo entra antes del minuto 1 de la canción y el 65% de  la canción es estrivillo. Esto de un gran estrivillo que se repite. La canción no tiene ni puente.  Hay otras estrategias que también están muy bien para conseguir que el estrivillo  se garrapido. Por ejemplo, ya que se han hecho tantos paralelismos por la temática,  la desaquíra y bizarra empieza con el estrivillo directamente.  Simplemente está con un blue pass filter. El estrivillo se llegó como de lejos, pero ya  según suena ya está soyendo la melodía y dice, ¿vale? ¿Estás ya sé cuál es ya?  Le he reconocido ya estoy. La siguiente revolución en la industria será conseguir que escuches  el estrivillo antes que la canción.  Precisamente eso es lo que ha hecho Miley. Se ha el estrivillo de Miley lo llevas escuchando  de 2012 en la estrofa de Bruno Mars. ¿Qué perdón?  Se encorce, melodice, vale entonces. Vamos a ver.  O sea que sí, la letra y la música están cojidas de Bruno Mars y sin embargo,  Bruno Mars no aparece en los créditos de composición, que es algo que podría haber ocurrido,  podrían hablar de blano y estoy escogiendo elementos de tu canción. Es posible que esté  protegido por Fairviews porque al final está respondiendo a la canción, no es una coge.  Igual que padre de familia pudo hacer una parodia de Star Wars sin pagar derechos a los  lucas. Esto entra dentro del marco, parodia, está comentando sobre la canción de Bruno Mars.  No tendría por qué pagarle. No es lo mismo que subir acogido, simplemente la melodía  de Bruno Mars para hacer otra canción que no tuvieran nada que ver.  Entonces formas que es lo más probable que haya ocurrido, que por diferencia ya la  ya llamable haya dicho, ¿hoche? Voy a hacer una canción que va a ser el primer single de  mi nuevo disco, literalmente, respondiéndote. Entonces estamos chill y el ya ha dicho, tranquila  a lo que quieras y ya está.  Que ahora la canción está arrasando y es posible que eso lleve a que haya cambios en  los créditos de la canción en algún momento del futuro porque se metan abogados discográficas.  Puede ser, pero vamos. Si eso ocurre ya lo comentaremos, no creo.  ¡Daba! ¡Tabos muy importante!  Si quieres aprender a acompañar al piano Flowers, When I was your man, When I was your  wife, puedes aprender a tocar el piano conmigo en muisejax.com.  Creo que voy a hacer un vídeo entero analizando la de When I was your man de Bruno Mars,  porque me flipa y se merece su propio análisis. Espero que os haya gustado y nos vemos la semana  que viene. Hasta pronto.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 130, but your input_length is only 88. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Flowers de My Liz Iris\" es de la canción más estremeada en su primera semana en la historia of Spotify. \"Quiero analizar Flowers porque me gusta mucho\" My Liz Iris es una canción respuesta en todas las maneras de responder letra, melodía y acordes. A gente que se dedica a analizar música y siendo no tiene nada que ver melodicamente, no es verdad. Miley: 'I don't ever want to have to need someone again where you feel like without them,  you can't be yourself' Esta letra es en respuesta a cómo se sentía Miley en su relación with su ex.  x, está articulándolo como una respuesta,  a una canción de Bruno Mars. Cantadas desde la perspectiva del chico en la relación, diciendo esto. H get yelled. La melodía y los acordes del estribillo de Miley son la estrofe de  the canción de Bruno Mars. Los acordes son los mismos. Si veinte Bruno adornan algunos acordes metiéndole  a una séptima in medio. Bruno hace.  un poco más, porque está todo más dilatado. Bruno es una esencia también hace la menor re, sol, do and para ir del do, a the menor, hace el  solo, y a la womenor hace prácticamente lo mismo. Miley ha cogido Bruno  Mars, lo ha hecho el doble de rápido and ha cantado lo that cantaba Bruno prácticamente. Es el tema de Flágo y de Undertale por encima en Flogueres de Miley. Lo único que cambia es el tempo. I will Survive tiene esa serie de acordes. Puedes coger los cuatro primeros acordes y hacer las cuatro estaciones Miley, Sairos muy fácilmente. La segunda mitad de la estribuía de Flowers suena muy  parecida a estas cuerdas de I will Survive. Está como respondiendo  a Bruno Mars. The R, el artista y repertorio, es  un puesto de una persona que se supone que ve la por tu éxito profesional. Tú le enseñas esta canción a the R y la R te pone un 10. Te pone  una matrícula de honor  y es por the estructura. El estrivillo entra en el segundo 33. Eso es Gold Standard. La canción no tiene ni puente. Hay otras estrategias que también están muy bien. El estrivillo se llegó como de lejos, pero ya  según suena ya está soyendo la melodía. Se han hecho tantos paralelismos por la temática. La siguiente revolución en la industria será conseguir que escuches  el estrivillos antes que la canción. Precisamente eso es lo that ha hecho Miley. Bruno Mars no aparece en los créditos de composición, que es algo que podría haber ocurrido. Es posible que esté  protegido by Fairviews porque al final está respondiendo a the canción, no es una coge. \"Voy a hacer una canción que va a ser el primer single de  mi nuevo danza\" \"No tendría por qué pagarle. No es lo mismo que subir acogido\" Es posible que haya cambios en  los créditos de la canción en algún momento del futuro porque se metan abogados discográficas. Si eso ocurre ya lo comentaremos, no creo. Voy a hacer un vídeo entero analizando la de When I was your man de Bruno Mars. Espero que os haya gustado y nos vemos la semana  that viene. Hasta pronto.\n",
      "Palabras clave que deben aparecer: ['canción', 'letra', 'Bruno', 'Mars', 'Miley', 'Cyrus', 'estribillo', 'acorde', 'melodía', 'flowers']\n",
      "Número de palabras clave: 10\n",
      "Conteo de palabras clave que aparecen: {'canción': 25, 'letra': 11, 'Bruno': 21, 'Mars': 16, 'Miley': 13, 'Cyrus': 0, 'estribillo': 3, 'acorde': 11, 'melodía': 8, 'flowers': 9}\n",
      "Número de palabras clave que aparecen: 9\n"
     ]
    }
   ],
   "source": [
    "transcripcion = transcribir_audio(\"./grabaciones/altozano.mp3\")\n",
    "print(transcripcion)\n",
    "resumen = summarize_long_text(transcripcion)\n",
    "print(resumen)\n",
    "palabras_clave = [\"canción\", \"letra\", \"Bruno\", \"Mars\", \"Miley\", \"Cyrus\", \"estribillo\", \"acorde\", \"melodía\", \"flowers\"]\n",
    "aparicion_palabras_clave(transcripcion, palabras_clave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validación de video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 130, but your input_length is only 128. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER: \n",
      "Describe el video. ASSISTANT: El video mostra a una persona masculina amb una barba llarga i cabellons llargs, vestida amb una camisa sense mànega. La persona es veu sorrient i es mou la mà per la cara, possiblement per a capturar el moment. La càmera es manté en moviment mentre la persona es mou la mà i es fa una expressió de sorpresa o de felicitat. La persona sembla estar\n",
      "El video mostra a una persona masculina amb una barba llarga i cabellons llargs. La persona es veu sorrient i es mou la mà per la cara, possiblement per a capturar el moment.\n",
      "Palabras clave que deben aparecer: ['catalán', 'dedoringa', 'hablar']\n",
      "Número de palabras clave: 3\n",
      "Conteo de palabras clave que aparecen: {'catalán': 0, 'dedoringa': 0, 'hablar': 0}\n",
      "Número de palabras clave que aparecen: 0\n",
      "Palabras clave que deben aparecer: ['man', 'room', 'speaking']\n",
      "Número de palabras clave: 3\n",
      "Conteo de palabras clave que aparecen: {'man': 1, 'room': 0, 'speaking': 0}\n",
      "Número de palabras clave que aparecen: 1\n"
     ]
    }
   ],
   "source": [
    "transcripcion = transcribir_video(\"./grabaciones/jordiwild.mp4\")\n",
    "print(transcripcion)\n",
    "resumen = summarize_long_text(transcripcion)\n",
    "print(resumen)\n",
    "\n",
    "palabras_clave_audio = [\"catalán\", \"dedoringa\", \"hablar\"]\n",
    "palabras_clave_video = [\"man\", \"room\", \"speaking\"]\n",
    "\n",
    "aparicion_palabras_clave(transcripcion, palabras_clave_audio)\n",
    "aparicion_palabras_clave(transcripcion, palabras_clave_video)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m transcripcion = \u001b[43mtranscribir_video\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./grabaciones/caza.mp4\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(transcripcion)\n\u001b[32m      3\u001b[39m resumen = summarize_long_text(transcripcion)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mtranscribir_video\u001b[39m\u001b[34m(video_path)\u001b[39m\n\u001b[32m     51\u001b[39m clip = read_video_pyav(container, indices)\n\u001b[32m     52\u001b[39m inputs_video = processor(text=prompt, videos=clip, padding=\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(model.device)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs_video\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m transcription = processor.decode(output[\u001b[32m0\u001b[39m][\u001b[32m2\u001b[39m:], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m transcription\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\Desktop\\AINE_AUDIO_SUMMARIZE\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\Desktop\\AINE_AUDIO_SUMMARIZE\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2465\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2457\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2458\u001b[39m         input_ids=input_ids,\n\u001b[32m   2459\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2460\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2461\u001b[39m         **model_kwargs,\n\u001b[32m   2462\u001b[39m     )\n\u001b[32m   2464\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2465\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2466\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2470\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2472\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2473\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2476\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2477\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2478\u001b[39m         input_ids=input_ids,\n\u001b[32m   2479\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2480\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2481\u001b[39m         **model_kwargs,\n\u001b[32m   2482\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\Desktop\\AINE_AUDIO_SUMMARIZE\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:3431\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3428\u001b[39m model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_hidden_states\u001b[39m\u001b[33m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m   3430\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[32m-> \u001b[39m\u001b[32m3431\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3432\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\Desktop\\AINE_AUDIO_SUMMARIZE\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\Desktop\\AINE_AUDIO_SUMMARIZE\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\Desktop\\AINE_AUDIO_SUMMARIZE\\.venv\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\Desktop\\AINE_AUDIO_SUMMARIZE\\.venv\\Lib\\site-packages\\transformers\\models\\llava_next_video\\modeling_llava_next_video.py:752\u001b[39m, in \u001b[36mLlavaNextVideoForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_ids, pixel_values, pixel_values_videos, image_sizes, attention_mask, position_ids, past_key_values, inputs_embeds, vision_feature_layer, vision_feature_select_strategy, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **lm_kwargs)\u001b[39m\n\u001b[32m    749\u001b[39m     video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n\u001b[32m    750\u001b[39m     inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, video_features)\n\u001b[32m--> \u001b[39m\u001b[32m752\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlanguage_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    754\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    755\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    756\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    757\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    758\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    759\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    761\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    762\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    766\u001b[39m logits = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    768\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\Desktop\\AINE_AUDIO_SUMMARIZE\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\Desktop\\AINE_AUDIO_SUMMARIZE\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\Desktop\\AINE_AUDIO_SUMMARIZE\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\Desktop\\AINE_AUDIO_SUMMARIZE\\.venv\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\Desktop\\AINE_AUDIO_SUMMARIZE\\.venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:821\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    816\u001b[39m output_hidden_states = (\n\u001b[32m    817\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    818\u001b[39m )\n\u001b[32m    820\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    834\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    835\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\Desktop\\AINE_AUDIO_SUMMARIZE\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\Desktop\\AINE_AUDIO_SUMMARIZE\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\Desktop\\AINE_AUDIO_SUMMARIZE\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\Desktop\\AINE_AUDIO_SUMMARIZE\\.venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:571\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    559\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    560\u001b[39m         partial(decoder_layer.\u001b[34m__call__\u001b[39m, **flash_attn_kwargs),\n\u001b[32m    561\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    568\u001b[39m         position_embeddings,\n\u001b[32m    569\u001b[39m     )\n\u001b[32m    570\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\Desktop\\AINE_AUDIO_SUMMARIZE\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\Desktop\\AINE_AUDIO_SUMMARIZE\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\Desktop\\AINE_AUDIO_SUMMARIZE\\.venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:318\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    315\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    317\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m hidden_states, self_attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    329\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\Desktop\\AINE_AUDIO_SUMMARIZE\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\Desktop\\AINE_AUDIO_SUMMARIZE\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\Desktop\\AINE_AUDIO_SUMMARIZE\\.venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:253\u001b[39m, in \u001b[36mLlamaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[39m\n\u001b[32m    250\u001b[39m hidden_shape = (*input_shape, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.head_dim)\n\u001b[32m    252\u001b[39m query_states = \u001b[38;5;28mself\u001b[39m.q_proj(hidden_states).view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m key_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mk_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m.view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    254\u001b[39m value_states = \u001b[38;5;28mself\u001b[39m.v_proj(hidden_states).view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    256\u001b[39m cos, sin = position_embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\Desktop\\AINE_AUDIO_SUMMARIZE\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\Desktop\\AINE_AUDIO_SUMMARIZE\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\Desktop\\AINE_AUDIO_SUMMARIZE\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "transcripcion = transcribir_video(\"./grabaciones/caza.mp4\")\n",
    "print(transcripcion)\n",
    "resumen = summarize_long_text(transcripcion)\n",
    "print(resumen)\n",
    "palabras_clave_video = [\"hunt\", \"prey\", \"lion\", \"giraffe\", \"tree\"]\n",
    "aparicion_palabras_clave(transcripcion, palabras_clave_video)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
